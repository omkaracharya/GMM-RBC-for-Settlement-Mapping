{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMM brainstorming\n",
    "\n",
    "## A basic approach to GMM implementation\n",
    "* GMM is implemented just as a collection of functions that can be contained in a single source file, nothing more. No OOP is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline:\n",
    "### Step 1:  Define log likelihood function that helps determine when EM algorithm \"converges\"\n",
    "* Inputs would be the data matrix and the current model parameters\n",
    "* Determine how likely is it that the data was generated by the mixture of Gaussians described by our model parameters?  Obviously, the higher the better. When the increase of this is too small, EM algorithm \"converges.\"\n",
    "* QUESTION TO PONDER: Is there an alternative metric besides log likelihood to determine convergence?\n",
    "\n",
    "### Step 2:  Define function for E-step (assign component responsibilities, given the current model parameters)\n",
    "* This function computes the component responsibilities (for each observation x_i, determine the responsibility that components 1,2,...,K take for x_i), which is what the E-step is.\n",
    "* Inputs would be the data matrix and the current model parameters (the component weights, the component means, the component covariance matrices) \n",
    "* For each observation x_i, the component responsibilities should sum to 1 (include assertion for this?)\n",
    "\n",
    "### Step 3:  Implement M-step (update the model parameters, given the current component responsibilities)\n",
    "* Step 3a:  Define function to compute soft counts (for each component j, sum the responsibilities that j takes for all the data points)\n",
    "* Step 3b:  Define function to update the component weights\n",
    "* Step 3c:  Define function to update the component means\n",
    "* Step 3d:  Define function to update the component covariance matrices\n",
    "* The M-step is steps 3b) thru 3d), with 3a) serving as a helper for the other steps\n",
    "\n",
    "### Step 4:  Write EM function\n",
    "* Inputs: Data matrix, epsilon for convergence\n",
    "* QUESTION TO PONDER: How to initialize EM algorithm?  Possibilites: Same way that we did random initialization for K-means, initialize from K-means solution, initialize using K-means++\n",
    "* Uses functions from Steps 1), 2), and 3b)-3d) as helper functions\n",
    "* Do the back-and-forth between the E-step and the M-step until convergence (see Step #1), may want to have maximum # iterations for the sake of debugging inadvertent non-covergence\n",
    "\n",
    "### Step 5:  Apply EM function to the UCI ML Repository's Iris Data Set? (as a sanity check?)\n",
    "* True, the Iris Data Set is already labeled (unlike a real unsupervised clustering scenario), however, we can use the labels for sanity checking\n",
    "* True, we already know beforehand that the number of components should be 3 (unlike a real unsupervised clustering scenario), but again, perhaps we could use this for sanity checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
